{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA6GQMyeW0Uw"
      },
      "source": [
        "# Ficha de Análise Léxica (Lex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEG4admeW0U6"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4RniEHLW0U7"
      },
      "source": [
        "A análise léxica é o processo de conversão de uma sequência de caracteres numa sequência de *tokens*, em que cada *token* representa uma unidade significativa da linguagem à qual os caracteres pertencem.\n",
        "\n",
        "Em Python, podemos fazer análise léxica de várias formas. A que iremos utilizar nas aulas recorre ao módulo Ply, que para além de análise léxica vai-nos permitir fazer análise sintática.\n",
        "\n",
        "Antes de usar o módulo Ply, precisamos de o instalar. Para isso, podemos usar o comando seguinte:\n",
        "\n",
        "```sh\n",
        "$ pip install ply\n",
        "```\n",
        "\n",
        "Depois, apenas precisamos de importar a ferramenta `lex.py` no nosso programa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P0GydwYDW0U9"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZReSLPaXW0U_"
      },
      "source": [
        "A primeira coisa que o nosso analisador léxico (ou *lexer*/*tokenizer*) precisa de ter é uma lista de *tokens*. Como exemplo, vamos definir um *lexer* que lê expressões aritméticas, como \"4 * (2 + 3)\". Neste exemplo já somos capazes de identificar alguns *tokens*..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G2wkBD06W0VA"
      },
      "outputs": [],
      "source": [
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    'MINUS',\n",
        "    'DIVIDE',\n",
        "    'TIMES',\n",
        "    'PAROPEN',\n",
        "    'PARCLOSE'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXQBfgZyW0VB"
      },
      "source": [
        "A seguir é preciso especificar cada *token*. Por outras palavras, precisamos de definir expressões regulares que permitam ao *tokenizer* identificar os *tokens*. Podemos fazê-lo através de variáveis ou de funções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iEQ1YAd2W0VC"
      },
      "outputs": [],
      "source": [
        "t_PLUS = r'\\+'\n",
        "t_PLUS = r'\\+'\n",
        "t_TIMES = r'\\*'\n",
        "t_DIVIDE = r'\\/'\n",
        "t_PAROPEN = r'\\('\n",
        "t_PARCLOSE = r'\\)'\n",
        "\n",
        "def t_NUMBER(t):\n",
        "    r'-?\\d+'\n",
        "    t.value = int(t.value)\n",
        "    return t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pvb5LHrW0VE"
      },
      "source": [
        "Podemos especificar um conjunto de caracteres que o analisador léxico vai ignorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6DjV8f5iW0VF"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Po4o75dW0VG"
      },
      "source": [
        "Precisamos ainda de definir o comportamento do *tokenizer* caso encontre um carácter ou sequência de caracteres que não corresponda a nenhum *token* conhecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_BJPu3DEW0VG"
      },
      "outputs": [],
      "source": [
        "def t_error(t):\n",
        "    print(f\"Carácter ilegal {t.value[0]}\")\n",
        "    t.lexer.skip(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIPl7tXW0VH"
      },
      "source": [
        "Agora, já somos capazes de construir o nosso analisador léxico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EwXxp_MlW0VI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Rule 't_RETS' defined for an unspecified token RETS\n",
            "ERROR: Rule 't_WORD' defined for an unspecified token WORD\n",
            "ERROR: Rule 't_COMMA' defined for an unspecified token COMMA\n",
            "ERROR: Rule 't_DOT' defined for an unspecified token DOT\n",
            "ERROR: Rule 't_E_MARK' defined for an unspecified token E_MARK\n",
            "ERROR: Rule 't_Q_MARK' defined for an unspecified token Q_MARK\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "<module '__main__'> is a built-in module",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lexer \u001b[39m=\u001b[39m lex\u001b[39m.\u001b[39;49mlex()\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\ply\\lex.py:908\u001b[0m, in \u001b[0;36mlex\u001b[1;34m(module, object, debug, optimize, lextab, reflags, nowarn, outputdir, debuglog, errorlog)\u001b[0m\n\u001b[0;32m    906\u001b[0m linfo\u001b[39m.\u001b[39mget_all()\n\u001b[0;32m    907\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m optimize:\n\u001b[1;32m--> 908\u001b[0m     \u001b[39mif\u001b[39;00m linfo\u001b[39m.\u001b[39;49mvalidate_all():\n\u001b[0;32m    909\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mSyntaxError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt build lexer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m optimize \u001b[39mand\u001b[39;00m lextab:\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\ply\\lex.py:579\u001b[0m, in \u001b[0;36mLexerReflect.validate_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_tokens()\n\u001b[0;32m    578\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_literals()\n\u001b[1;32m--> 579\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_rules()\n\u001b[0;32m    580\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\ply\\lex.py:821\u001b[0m, in \u001b[0;36mLexerReflect.validate_rules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m--> 821\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_module(module)\n",
            "File \u001b[1;32md:\\python\\lib\\site-packages\\ply\\lex.py:833\u001b[0m, in \u001b[0;36mLexerReflect.validate_module\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_module\u001b[39m(\u001b[39mself\u001b[39m, module):\n\u001b[0;32m    832\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 833\u001b[0m         lines, linen \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetsourcelines(module)\n\u001b[0;32m    834\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m:\n\u001b[0;32m    835\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[1;32md:\\python\\lib\\inspect.py:1129\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m \n\u001b[0;32m   1123\u001b[0m \u001b[39mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m unwrap(\u001b[39mobject\u001b[39m)\n\u001b[1;32m-> 1129\u001b[0m lines, lnum \u001b[39m=\u001b[39m findsource(\u001b[39mobject\u001b[39;49m)\n\u001b[0;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m istraceback(\u001b[39mobject\u001b[39m):\n\u001b[0;32m   1132\u001b[0m     \u001b[39mobject\u001b[39m \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39mtb_frame\n",
            "File \u001b[1;32md:\\python\\lib\\inspect.py:940\u001b[0m, in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfindsource\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[0;32m    933\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the entire source file and starting line number for an object.\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m \u001b[39m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[39m    or code object.  The source code is returned as a list of all the lines\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[39m    in the file and the line number indexes a line in that list.  An OSError\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m    is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 940\u001b[0m     file \u001b[39m=\u001b[39m getsourcefile(\u001b[39mobject\u001b[39;49m)\n\u001b[0;32m    941\u001b[0m     \u001b[39mif\u001b[39;00m file:\n\u001b[0;32m    942\u001b[0m         \u001b[39m# Invalidate cache if needed.\u001b[39;00m\n\u001b[0;32m    943\u001b[0m         linecache\u001b[39m.\u001b[39mcheckcache(file)\n",
            "File \u001b[1;32md:\\python\\lib\\inspect.py:817\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetsourcefile\u001b[39m(\u001b[39mobject\u001b[39m):\n\u001b[0;32m    814\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the filename that can be used to locate an object's source.\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[39m    Return None if no way can be identified to get the source.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 817\u001b[0m     filename \u001b[39m=\u001b[39m getfile(\u001b[39mobject\u001b[39;49m)\n\u001b[0;32m    818\u001b[0m     all_bytecode_suffixes \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mDEBUG_BYTECODE_SUFFIXES[:]\n\u001b[0;32m    819\u001b[0m     all_bytecode_suffixes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mOPTIMIZED_BYTECODE_SUFFIXES[:]\n",
            "File \u001b[1;32md:\\python\\lib\\inspect.py:778\u001b[0m, in \u001b[0;36mgetfile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mobject\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n\u001b[1;32m--> 778\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is a built-in module\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mobject\u001b[39m))\n\u001b[0;32m    779\u001b[0m \u001b[39mif\u001b[39;00m isclass(\u001b[39mobject\u001b[39m):\n\u001b[0;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mobject\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m'\u001b[39m):\n",
            "\u001b[1;31mTypeError\u001b[0m: <module '__main__'> is a built-in module"
          ]
        }
      ],
      "source": [
        "lexer = lex.lex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFhRKYNYW0VI"
      },
      "source": [
        "Para o usar, precisamos de lhe dar algum valor de *input* e depois pedir-lhe para ir devolvendo os *tokens* que encontrar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46_TGFZcW0VJ"
      },
      "outputs": [],
      "source": [
        "data = '''\n",
        "3 + 4 * 10\n",
        "  + -20 *2\n",
        "'''\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tkRAn_9W0VJ"
      },
      "source": [
        "É possível consultar a documentação do *lex.py* em https://ply.readthedocs.io/en/latest/ply.html#lex. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPnsQyj8W0VK"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72xXUjvvW0VK"
      },
      "source": [
        "### 1. Frases\n",
        "\n",
        "Define um analisador léxico capaz de ler uma frase e de identificar os seus componentes (palavras, vírgulas, sinais de pontuação)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6KA_i8LW0VL"
      },
      "source": [
        "### 2. Listas Mistas\n",
        "\n",
        "Define um analisador léxico capaz de receber listas números, palavras ou valores booleanos como input (e.g.: `[ 1,5, palavra, False ,3.14,   0, fim]`) e identificar os seus *tokens*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7mHMGTSW0VL"
      },
      "source": [
        "### 3. JSON\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato JSON e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um documento JSON:\n",
        "\n",
        "---\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"age\": 21,\n",
        "  \"gender\": \"male\",\n",
        "  \"height\": 1.68,\n",
        "  \"address\": {\n",
        "    \"street\": \"123 Main Street\",\n",
        "    \"city\": \"New York\",\n",
        "    \"country\": \"USA\",\n",
        "    \"zip\": \"10001\"\n",
        "  },\n",
        "  \"married\": false,\n",
        "  \"hobbies\": [\n",
        "    {\n",
        "      \"name\": \"reading\",\n",
        "      \"books\": [\n",
        "        {\n",
        "          \"title\": \"Heartstopper: Volume 1\",\n",
        "          \"author\": \"Alice Oseman\",\n",
        "          \"genres\": [\"Graphic Novels\", \"Romance\", \"Queer\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"1984\",\n",
        "          \"author\": \"George Orwell\",\n",
        "          \"genres\": [\"Science Fiction\", \"Dystopia\", \"Politics\"]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"gaming\",\n",
        "      \"games\": [\n",
        "        {\n",
        "          \"title\": \"Portal 2\",\n",
        "          \"platform\": [\"PC\", \"PlayStation 3\", \"Xbox 360\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"Synth Riders\", \n",
        "          \"platform\": [\"PSVR\", \"PSVR2\", \"PCVR\", \"Oculus Quest\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSpix9zkW0VM"
      },
      "source": [
        "## Condições de contexto\n",
        "\n",
        "Para certos analisadores léxico, pode ser útil ter diferentes estados. Por exemplo, se definirmos um analisador léxico para um ficheiro XML, pode ser útil verificar se o nome usado para fechar uma *tag* foi o mesmo que foi usado para a abrir.\n",
        "\n",
        "Exemplo de parte de um ficheiro XML:\n",
        "\n",
        "```xml\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bF_G-gWW0VN"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "states = (\n",
        "    ('taga', 'exclusive'),\n",
        "    ('tagf', 'exclusive'), # num estado exclusivo, apenas aplicamos os tokens e regras para esse estado\n",
        "                           # por outro lado, num estado inclusivo, as regras e tokens desse estado juntam-se às outras regras e tokens\n",
        "                           # o estado inicial chama-se 'INITIAL' e não é preciso defini-lo\n",
        ")\n",
        "\n",
        "tokens = (\n",
        "    'ABRIR_TAG',\n",
        "    'ABRIR_TAG_F',\n",
        "    'FECHAR_TAG',\n",
        "    'NOME_TAG',\n",
        "    'VALOR'\n",
        ")\n",
        "\n",
        "t_ignore = ' \\t\\n' # estes tokens apenas são ignorados no estado 'INITIAL' e em estados inclusivos\n",
        "\n",
        "t_VALOR = r'[^<]+'\n",
        "\n",
        "def t_ABRIR_TAG_F(t):\n",
        "    r'</'\n",
        "    t.lexer.begin('tagf') # entramos no estado 'tagf'\n",
        "    return t\n",
        "\n",
        "def t_ABRIR_TAG(t):\n",
        "    r'<'\n",
        "    t.lexer.begin('taga') # entramos no estado 'taga'\n",
        "    return t\n",
        "\n",
        "def t_taga_tagf_FECHAR_TAG(t):\n",
        "    r'>'\n",
        "    t.lexer.begin('INITIAL') # voltamos ao estado inicial\n",
        "    return t\n",
        "\n",
        "def t_taga_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    t.lexer.stack.append(t.value)\n",
        "    return t\n",
        "\n",
        "def t_tagf_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    if len(t.lexer.stack) > 0:\n",
        "        if (nt := t.lexer.stack.pop(-1)) != t.value:\n",
        "            print(f\"Erro - esperado nome de tag '{nt}', mas foi lido '{t.value}'!\")\n",
        "    else:\n",
        "        print(\"Erro - nenhuma tag aberta!\")\n",
        "    return t\n",
        "\n",
        "def t_ANY_error(t): # regra válida para todos os estados\n",
        "    print(f\"Carácter ilegal: {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "\n",
        "data = '''\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "'''\n",
        "\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.stack = list() # vamos usar esta lista como stack para verificar os nomes das tags\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqLA6pKXW0VO"
      },
      "source": [
        "Se quisermos manter informação sobre as linhas nas quais os *tokens* aparecem, podemos usar o atributo `lineno`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7acWM5irW0VO"
      },
      "outputs": [],
      "source": [
        "t_ignore = r' \\t'\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyrts-DkW0VP"
      },
      "source": [
        "## Exercícios 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YLazQoqW0VP"
      },
      "source": [
        "### 1. BibTeX\n",
        "\n",
        "Define um analisador léxico capaz de ler um ficheiro no formato *BibTeX* e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um ficheiro BibTeX:\n",
        "\n",
        "---\n",
        "\n",
        "```bibtex\n",
        "@incollection {HDYE78,\n",
        "author = \"Ricardo Martini and Pedro Rangel Henriques and Giovani Libreloto\",\n",
        "title = \"Storing Archival Emigration Documents to Create Virtual Exhibition Rooms\",\n",
        "booktitle = \"New Contributions in Information Systems and Technologies\",\n",
        "series=\"Advances in Intelligent Systems and Computing\",\n",
        "editor=\"Rocha, Alvaro and Correia, Ana and Costanzo, S. and Reis, Luis Paulo\",\n",
        "volume=\"353\",\n",
        "pages=\"403-409\",\n",
        "year = \"2015\",\n",
        "month =  \"April\"\n",
        "}\n",
        "\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = {An AST-based tool, Spector, for Plagiarism Detection},\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundacion General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}}\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = \"{A}n {AST}-based tool, {S}pector, for Plagiarism Detection\",\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundaci ́on General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyRXYkysW0VQ"
      },
      "source": [
        "### 2. Markdown\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato Markdown e identificar os seus *tokens*. O *tokenizer* deve conseguir identificar pelo menos:\n",
        "\n",
        "- cabeçalhos\n",
        "- parágrafos\n",
        "- listas\n",
        "- texto itálico\n",
        "- texto negrito\n",
        "- blocos de código\n",
        "- citações\n",
        "\n",
        "Exemplo de um documento Markdown:\n",
        "\n",
        "---\n",
        "\n",
        "````markdown\n",
        "# This is a heading\n",
        "\n",
        "## This is a subheading\n",
        "\n",
        "This is some **bold** text.\n",
        "\n",
        "This is some *italic* text.\n",
        "\n",
        "- This is a bullet point\n",
        "- This is another bullet point\n",
        "\n",
        "1. This is a numbered list\n",
        "2. This is another numbered list item\n",
        "\n",
        "> This is a blockquote.\n",
        "\n",
        "`This is some inline code.`\n",
        "\n",
        "```python\n",
        "# This is some code block\n",
        "print(\"Hello, world!\")\n",
        "```\n",
        "````\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtqdrjKWW0VR"
      },
      "source": [
        "### 3. Somador on/off\n",
        "\n",
        "Cria um programa em Python que tenha o seguinte comportamento:\n",
        "\n",
        "* Pretende-se um programa que some todas as sequências de dígitos que encontre num texto;\n",
        "* Prepara o programa para ler o texto do canal de entrada: stdin;\n",
        "* Sempre que encontrar a string “Off” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é desligado;\n",
        "* Sempre que encontrar a string “On” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é novamente ligado;\n",
        "* Sempre que encontrar o caráter “=”, o resultado da soma é colocado na saída.\n",
        "\n",
        "Este exercício já foi proposto como TPC, mas agora deves tentar resolvê-lo usando um analisador léxico com condições de contexto."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5994ec7557f9f4a3c4b5d4f7132f657ee5f793fd2b9e6534077474582be4b489"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
