{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdHi2G0OlbE"
      },
      "source": [
        "# Ficha de Análise Léxica (Lex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0JdE83EOlb0"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC00B5-IOlb1"
      },
      "source": [
        "A análise léxica é o processo de conversão de uma sequência de caracteres numa sequência de *tokens*, em que cada *token* representa uma unidade significativa da linguagem à qual os caracteres pertencem.\n",
        "\n",
        "Em Python, podemos fazer análise léxica de várias formas. A que iremos utilizar nas aulas recorre ao módulo Ply, que para além de análise léxica vai-nos permitir fazer análise sintática.\n",
        "\n",
        "Antes de usar o módulo Ply, precisamos de o instalar. Para isso, podemos usar o comando seguinte:\n",
        "\n",
        "```sh\n",
        "$ pip install ply\n",
        "```\n",
        "\n",
        "Depois, apenas precisamos de importar a ferramenta `lex.py` no nosso programa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yXKYfAnOlcJ"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx_dqWwkOlcL"
      },
      "source": [
        "A primeira coisa que o nosso analisador léxico (ou *lexer*/*tokenizer*) precisa de ter é uma lista de *tokens*. Como exemplo, vamos definir um *lexer* que lê expressões aritméticas, como \"4 * (2 + 3)\". Neste exemplo já somos capazes de identificar alguns *tokens*..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXrbPicOlcM"
      },
      "outputs": [],
      "source": [
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    # completar...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNXH-ZeOlcM"
      },
      "source": [
        "A seguir é preciso especificar cada *token*. Por outras palavras, precisamos de definir expressões regulares que permitam ao *tokenizer* identificar os *tokens*. Podemos fazê-lo através de variáveis ou de funções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95mz8RYOlcb"
      },
      "outputs": [],
      "source": [
        "t_PLUS = r'\\+'\n",
        "# completar...\n",
        "\n",
        "def t_NUMBER(t):\n",
        "    # completar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2ILFtkOlcc"
      },
      "source": [
        "Podemos especificar um conjunto de caracteres que o analisador léxico vai ignorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fxalLuTOlcd"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9xt-tqOlce"
      },
      "source": [
        "Precisamos ainda de definir o comportamento do *tokenizer* caso encontre um carácter ou sequência de caracteres que não corresponda a nenhum *token* conhecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYqjFDTEOlcp"
      },
      "outputs": [],
      "source": [
        "def t_error(t):\n",
        "    print(f\"Carácter ilegal {t.value[0]}\")\n",
        "    t.lexer.skip(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hMOGfxsOldI"
      },
      "source": [
        "Agora, já somos capazes de construir o nosso analisador léxico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9vAe7UOldP"
      },
      "outputs": [],
      "source": [
        "lexer = lex.lex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjACXglAOldP"
      },
      "source": [
        "Para o usar, precisamos de lhe dar algum valor de *input* e depois pedir-lhe para ir devolvendo os *tokens* que encontrar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5BEZj6XOldQ"
      },
      "outputs": [],
      "source": [
        "data = '''\n",
        "3 + 4 * 10\n",
        "  + -20 *2\n",
        "'''\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrQMVnjOldQ"
      },
      "source": [
        "É possível consultar a documentação do *lex.py* em https://ply.readthedocs.io/en/latest/ply.html#lex. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyge6OSnOldR"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3hIQxn8OldR"
      },
      "source": [
        "### 1. Frases\n",
        "\n",
        "Define um analisador léxico capaz de ler uma frase e de identificar os seus componentes (palavras, vírgulas, sinais de pontuação)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UST_08V9OldS"
      },
      "source": [
        "### 2. Listas Mistas\n",
        "\n",
        "Define um analisador léxico capaz de receber listas com números, palavras ou valores booleanos como input (e.g.: `[ 1,5, palavra, False ,3.14,   0, fim]`) e identificar os seus *tokens*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s__Lct3Oldh"
      },
      "source": [
        "### 3. JSON\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato JSON e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um documento JSON:\n",
        "\n",
        "---\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"age\": 21,\n",
        "  \"gender\": \"male\",\n",
        "  \"height\": 1.68,\n",
        "  \"address\": {\n",
        "    \"street\": \"123 Main Street\",\n",
        "    \"city\": \"New York\",\n",
        "    \"country\": \"USA\",\n",
        "    \"zip\": \"10001\"\n",
        "  },\n",
        "  \"married\": false,\n",
        "  \"hobbies\": [\n",
        "    {\n",
        "      \"name\": \"reading\",\n",
        "      \"books\": [\n",
        "        {\n",
        "          \"title\": \"Heartstopper: Volume 1\",\n",
        "          \"author\": \"Alice Oseman\",\n",
        "          \"genres\": [\"Graphic Novels\", \"Romance\", \"Queer\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"1984\",\n",
        "          \"author\": \"George Orwell\",\n",
        "          \"genres\": [\"Science Fiction\", \"Dystopia\", \"Politics\"]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"gaming\",\n",
        "      \"games\": [\n",
        "        {\n",
        "          \"title\": \"Portal 2\",\n",
        "          \"platform\": [\"PC\", \"PlayStation 3\", \"Xbox 360\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"Synth Riders\", \n",
        "          \"platform\": [\"PSVR\", \"PSVR2\", \"PCVR\", \"Oculus Quest\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTLcLtKAOldi"
      },
      "source": [
        "## Condições de contexto\n",
        "\n",
        "Para certos analisadores léxico, pode ser útil ter diferentes estados. Por exemplo, se definirmos um analisador léxico para um ficheiro XML, pode ser útil verificar se o nome usado para fechar uma *tag* foi o mesmo que foi usado para a abrir.\n",
        "\n",
        "Exemplo de parte de um ficheiro XML:\n",
        "\n",
        "```xml\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDH2v3NjOldw"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "states = (\n",
        "    ('taga', 'exclusive'),\n",
        "    ('tagf', 'exclusive'), # num estado exclusivo, apenas aplicamos os tokens e regras para esse estado\n",
        "                           # por outro lado, num estado inclusivo, as regras e tokens desse estado juntam-se às outras regras e tokens\n",
        "                           # o estado inicial chama-se 'INITIAL' e não é preciso defini-lo\n",
        ")\n",
        "\n",
        "tokens = (\n",
        "    'ABRIR_TAG',\n",
        "    'ABRIR_TAG_F',\n",
        "    'FECHAR_TAG',\n",
        "    'NOME_TAG',\n",
        "    'VALOR'\n",
        ")\n",
        "\n",
        "t_ignore = ' \\t\\n' # estes tokens apenas são ignorados no estado 'INITIAL' e em estados inclusivos\n",
        "\n",
        "t_VALOR = r'[^<]+'\n",
        "\n",
        "def t_ABRIR_TAG_F(t):\n",
        "    r'</'\n",
        "    t.lexer.begin('tagf') # entramos no estado 'tagf'\n",
        "    return t\n",
        "\n",
        "def t_ABRIR_TAG(t):\n",
        "    r'<'\n",
        "    t.lexer.begin('taga') # entramos no estado 'taga'\n",
        "    return t\n",
        "\n",
        "def t_taga_tagf_FECHAR_TAG(t):\n",
        "    r'>'\n",
        "    t.lexer.begin('INITIAL') # voltamos ao estado inicial\n",
        "    return t\n",
        "\n",
        "def t_taga_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    t.lexer.stack.append(t.value)\n",
        "    return t\n",
        "\n",
        "def t_tagf_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    if len(t.lexer.stack) > 0:\n",
        "        if (nt := t.lexer.stack.pop(-1)) != t.value:\n",
        "            print(f\"Erro - esperado nome de tag '{nt}', mas foi lido '{t.value}'!\")\n",
        "    else:\n",
        "        print(\"Erro - nenhuma tag aberta!\")\n",
        "    return t\n",
        "\n",
        "def t_ANY_error(t): # regra válida para todos os estados\n",
        "    print(f\"Carácter ilegal: {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "\n",
        "data = '''\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "'''\n",
        "\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.stack = list() # vamos usar esta lista como stack para verificar os nomes das tags\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uvix55yOldz"
      },
      "source": [
        "Se quisermos manter informação sobre as linhas nas quais os *tokens* aparecem, podemos usar o atributo `lineno`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5CFURcLOleU"
      },
      "outputs": [],
      "source": [
        "t_ignore = r' \\t'\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJqjWeSfOleV"
      },
      "source": [
        "## Exercícios 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onMLXP8gOleY"
      },
      "source": [
        "### 1. BibTeX\n",
        "\n",
        "Define um analisador léxico capaz de ler um ficheiro no formato *BibTeX* e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um ficheiro BibTeX:\n",
        "\n",
        "---\n",
        "\n",
        "```bibtex\n",
        "@incollection {HDYE78,\n",
        "author = \"Ricardo Martini and Pedro Rangel Henriques and Giovani Libreloto\",\n",
        "title = \"Storing Archival Emigration Documents to Create Virtual Exhibition Rooms\",\n",
        "booktitle = \"New Contributions in Information Systems and Technologies\",\n",
        "series=\"Advances in Intelligent Systems and Computing\",\n",
        "editor=\"Rocha, Alvaro and Correia, Ana and Costanzo, S. and Reis, Luis Paulo\",\n",
        "volume=\"353\",\n",
        "pages=\"403-409\",\n",
        "year = \"2015\",\n",
        "month =  \"April\"\n",
        "}\n",
        "\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = {An AST-based tool, Spector, for Plagiarism Detection},\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundacion General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}}\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = \"{A}n {AST}-based tool, {S}pector, for Plagiarism Detection\",\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundaci ́on General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQCgKtROOlei"
      },
      "source": [
        "### 2. Markdown\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato Markdown e identificar os seus *tokens*. O *tokenizer* deve conseguir identificar pelo menos:\n",
        "\n",
        "- cabeçalhos\n",
        "- parágrafos\n",
        "- listas\n",
        "- texto itálico\n",
        "- texto negrito\n",
        "- blocos de código\n",
        "- citações\n",
        "\n",
        "Exemplo de um documento Markdown:\n",
        "\n",
        "---\n",
        "\n",
        "````markdown\n",
        "# This is a heading\n",
        "\n",
        "## This is a subheading\n",
        "\n",
        "This is some **bold** text.\n",
        "\n",
        "This is some *italic* text.\n",
        "\n",
        "- This is a bullet point\n",
        "- This is another bullet point\n",
        "\n",
        "1. This is a numbered list\n",
        "2. This is another numbered list item\n",
        "\n",
        "> This is a blockquote.\n",
        "\n",
        "`This is some inline code.`\n",
        "\n",
        "```python\n",
        "# This is some code block\n",
        "print(\"Hello, world!\")\n",
        "```\n",
        "````\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxXn9uTSOlep"
      },
      "source": [
        "### 3. Somador on/off\n",
        "\n",
        "Cria um programa em Python que tenha o seguinte comportamento:\n",
        "\n",
        "* Pretende-se um programa que some todas as sequências de dígitos que encontre num texto;\n",
        "* Prepara o programa para ler o texto do canal de entrada: stdin;\n",
        "* Sempre que encontrar a string “Off” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é desligado;\n",
        "* Sempre que encontrar a string “On” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é novamente ligado;\n",
        "* Sempre que encontrar o caráter “=”, o resultado da soma é colocado na saída.\n",
        "\n",
        "Este exercício já foi proposto como TPC, mas agora deves tentar resolvê-lo usando um analisador léxico com condições de contexto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQePKCJEg8Oj"
      },
      "source": [
        "### 4. Removedor de Comentários\n",
        "\n",
        "Desenvolve um analisador léxico capaz de ler um ficheiro de texto e ignorar todo o texto dentro de comentários inline (desde \"//\" até ao fim de linha) e todo o texto dentro de comentários multiline (desde \"/*\" até \"*/\").\n",
        "\n",
        "O *lexer* deve suportar convenientemente comentários dentro de comentários,   conforme exemplificado abaixo:\n",
        "\n",
        "---\n",
        "```bibtex\n",
        "/* comment */ ola1\n",
        "\n",
        "/* comment****comment */ ola2 /*\n",
        "comment\n",
        "/* comentário dentro de comentário */\n",
        "****/ ola3\n",
        "\n",
        "/*********/\n",
        "\n",
        "ola4\n",
        " mais um pouco // remover comentário inline\n",
        "FIM\n",
        "```\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JutNNzDGLWyQ"
      },
      "source": [
        "### 5. Rota do Coelho da Páscoa\n",
        "\n",
        "A Páscoa está a chegar, e o Coelho da Páscoa precisa da tua ajuda para entregar os ovos. Ele deu-te uma lista de locais onde tem de parar, mas alguns dos locais estão fechados durante a Páscoa. Precisas de determinar quais dos locais estão abertos e quais é que estão fechados.\n",
        "\n",
        "Os locais seguem o formato `\"nome do local (open/closed)\"`, e estão separados por vírgulas. Precisas de especificar um analisador léxico para esta lista que determine quais os locais que estão abertos.\n",
        "\n",
        "Para além disso, alguns dos locais têm decorações especiais para a Páscoa, que o Coelho quer ir ver. Estes locais estão assinalados com uma *flag* adicional na lista, no formato `\"nome do local (open/closed) [nome da decoração]\"`. Assim, o resultado final do *tokenizer* deve ser um dicionário que associe a cada local que está aberto a decoração presente no mesmo, ou `None` se esse local não estiver decorado.\n",
        "\n",
        "O *lexer* deve possuir estados para cada um dos tipos de valor a encontrar (nome de local, estado de abertura e decoração).\n",
        "\n",
        "Caso algum local apareça mais do que uma vez no *input* com um estado de abertura diferente (e.g.: aberto uma vez e fechado outra) o analisador léxico deve imprimir uma mensagem de erro e não incluir esse local no dicionário final.\n",
        "\n",
        "Segue-se a lista dos locais:\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "\"Springfield (open) [bunny statue]\",\n",
        "\"Happyville (closed)\", \n",
        "\"Chocolate Land (open) [Easter eggs]\",\n",
        "\"Rabbit Hill\n",
        "(closed) [chocolate bunnies]\",\n",
        "\"Cadbury Castle (open)\",\n",
        "\"Peeps Park (closed)\",\n",
        "\"Lily Gardens\n",
        "(open) [butterfly garden]\",\n",
        "\"Cocoa Town (open) [chocolate river]\",\n",
        "\"Carrot\n",
        "Cove (closed)\",\n",
        "\"Hoppington (open) [bunny hop]\",\n",
        "\"Jellybean Junction (closed)\",\n",
        "\"Bon Bon Bay (open) [marshmallow chickens]\",\n",
        "\"Marshmallow \n",
        "Marsh (closed)\",\n",
        "\"Candy Cane Canyon (open)\",\n",
        "\"Cotton Candy Corner \n",
        "(closed)\",\n",
        "\"Sugarplum Square (open) [sugarplum fairies]\",\n",
        "\"Pudding Plains (closed)\n",
        "[pudding cups]\",\n",
        "\"Gingerbread Village (open)\",\n",
        "\"Caramel Castle (closed)\n",
        "[caramel apples]\",\n",
        "\"Lollipop Lane (open)\"\n",
        "\"Chirping Forest (open)\",\n",
        "\"Carrot Farm (closed)\",\n",
        "\"Jellybean Junction \n",
        "(open) [jellybean trail]\",\n",
        "\"Springfield (closed)\",\n",
        "\"Chocolate Land \n",
        "(closed)\",\n",
        "\"Lily Gardens (closed)\"\n",
        "```\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5994ec7557f9f4a3c4b5d4f7132f657ee5f793fd2b9e6534077474582be4b489"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
